{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "QtGIBz0z-_t0",
      "metadata": {
        "id": "QtGIBz0z-_t0"
      },
      "source": [
        "### Instals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PUHbEX8nSPE_",
      "metadata": {
        "id": "PUHbEX8nSPE_"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "# Deletes 'CDTD-CFG' map if necessary\n",
        "shutil.rmtree('/content/CDTD-CFG', ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E5PpE9riSUPQ",
      "metadata": {
        "id": "E5PpE9riSUPQ"
      },
      "outputs": [],
      "source": [
        "# Clone git and move into the repo\n",
        "!git clone https://github.com/luethytommaso/CDTD-CFG.git\n",
        "%cd CDTD-CFG/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eSenW77DSV0u",
      "metadata": {
        "collapsed": true,
        "id": "eSenW77DSV0u"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\".\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TDtgTiKAZUYD",
      "metadata": {
        "collapsed": true,
        "id": "TDtgTiKAZUYD"
      },
      "outputs": [],
      "source": [
        "!pip install requirements.txt\n",
        "!pip install catboost\n",
        "!pip install torch\n",
        "!pip install sklearn\n",
        "!pip install scikit learn\n",
        "!pip install pandas\n",
        "!pip install seaborn\n",
        "!pip install torch_ema\n",
        "#torch-ema"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import random\n",
        "\n",
        "from cdtd_cfg import CDTD\n",
        "\n",
        "# set all seeds\n",
        "def set_seeds(seed, cuda_deterministic=False):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "            if cuda_deterministic:\n",
        "                print(\"cuda_deterministic = True\")\n",
        "                torch.backends.cudnn.deterministic = True\n",
        "                torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seeds(42, cuda_deterministic=True)"
      ],
      "metadata": {
        "id": "TvfQHKBm-9XC"
      },
      "id": "TvfQHKBm-9XC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zaCjm-eGs3V9",
      "metadata": {
        "id": "zaCjm-eGs3V9"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "from collections import Counter\n",
        "from scipy.stats import gaussian_kde\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy.stats import wasserstein_distance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import OrdinalEncoder, QuantileTransformer, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from IPython.display import display\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\"\"\"\n",
        "This code block contains all methods called outside of the CDTD and Synthpop frameworks throughout this analysis.\n",
        "\"\"\"\n",
        "\n",
        "def fit_cdtd_model(X_cat_train, X_cont_train, cfg, label_1_idx, label_2_idx, dropout_ratio=0.2, num_steps_train=3000, num_steps_warmup=1000):\n",
        "    # Extract the target conditioning variables (two categorical labels) from the training data\n",
        "    y_condition_1 = X_cat_train[:, label_1_idx]\n",
        "    y_condition_2 = X_cat_train[:, label_2_idx]\n",
        "\n",
        "    # Initialize the CDTD model with training data and conditioning labels\n",
        "    cdtd = CDTD(\n",
        "        X_cat_train = X_cat_train,\n",
        "        X_cont_train = X_cont_train,\n",
        "        cfg = cfg,\n",
        "        y_condition_1=y_condition_1,\n",
        "        y_condition_2=y_condition_2,\n",
        "    )\n",
        "\n",
        "    # Train the CDTD model with the given number of steps, warmup, and dropout\n",
        "    cdtd.fit(X_cat_train, X_cont_train, num_steps_train=num_steps_train, num_steps_warmup=num_steps_warmup, cfg = cfg,\n",
        "            y_condition_1=y_condition_1, y_condition_2=y_condition_2, dropout_ratio = dropout_ratio)\n",
        "\n",
        "    return cdtd\n",
        "\n",
        "\n",
        "def generate_samples(num_samples, cdtd_model, cat_int_enc, cont_enc, cont_std, cat_features, cont_features,\n",
        "                     int_features, cfg, cfg_scale_1, cfg_scale_2, label_1, label_2, target_label_1, target_label_2,\n",
        "                     X_cat_train, targeted_sampling=False, return_latents=False):\n",
        "\n",
        "    # Locate indices of the two conditioning variables in the categorical features\n",
        "    label_1_idx = cat_features.index(label_1)\n",
        "    label_2_idx = cat_features.index(label_2)\n",
        "    classes_label_1 = cat_int_enc.categories_[label_1_idx]\n",
        "    classes_label_2 = cat_int_enc.categories_[label_2_idx]\n",
        "    num_classes_1 = len(classes_label_1)\n",
        "    num_classes_2 = len(classes_label_2)\n",
        "\n",
        "    # Build probability matrix for sampling (ID vs OOD)\n",
        "    if cfg: # CFG denotes whether we call CDTD model or CDTD-CFG model\n",
        "        if targeted_sampling: # Targeted sampling denotes whether we call the CDTD-CFG model ID or OOD\n",
        "            # Set only the probability of the OOD class pair to 1 if CFG-OOD\n",
        "            target_label_1_idx = np.where(classes_label_1 == target_label_1)[0][0]\n",
        "            target_label_2_idx = np.where(classes_label_2 == target_label_2)[0][0]\n",
        "            probs_matrix = np.zeros((num_classes_1, num_classes_2))\n",
        "            probs_matrix[target_label_1_idx, target_label_2_idx] = 1.0\n",
        "        else:\n",
        "            # Copy the joint class distribution from training data for ID sampling\n",
        "            y_label_1_train = X_cat_train[:, label_1_idx].numpy()\n",
        "            y_label_2_train = X_cat_train[:, label_2_idx].numpy()\n",
        "            pair_counts = Counter(zip(y_label_1_train, y_label_2_train))\n",
        "            total = sum(pair_counts.values())\n",
        "            probs_matrix = np.zeros((num_classes_1, num_classes_2))\n",
        "            for (l1, l2), count in pair_counts.items():\n",
        "                probs_matrix[l1, l2] = count / total\n",
        "    else:\n",
        "        probs_matrix = None\n",
        "\n",
        "    # Inverse-transform categorical and continuous features back to original data space\n",
        "    X_cat_gen, X_cont_gen, latent_vectors = cdtd_model.sample(\n",
        "        num_samples=num_samples,\n",
        "        cfg=cfg,\n",
        "        probs_matrix=probs_matrix,\n",
        "        cfg_scale_1=cfg_scale_1,\n",
        "        cfg_scale_2=cfg_scale_2,\n",
        "        return_latents=return_latents\n",
        "    )\n",
        "\n",
        "    X_cat_gen_inv = cat_int_enc.inverse_transform(X_cat_gen)\n",
        "    X_cont_gen_inv = cont_std.inverse_transform(X_cont_gen)\n",
        "    X_cont_gen_inv = cont_enc.inverse_transform(X_cont_gen_inv)\n",
        "\n",
        "    # Create a DataFrame with the correct column names and data types\n",
        "    df_gen = pd.concat(\n",
        "        (pd.DataFrame(X_cat_gen_inv), pd.DataFrame(X_cont_gen_inv)), axis=1\n",
        "    )\n",
        "    df_gen.columns = cat_features + cont_features\n",
        "    df_gen[cont_features] = df_gen[cont_features].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    df_gen[int_features] = df_gen[int_features].round().astype(\"int\")\n",
        "    return df_gen, latent_vectors\n",
        "\n",
        "\n",
        "def plot_all_features(df_train, df_generated, cat_features, cont_features, cfg=False, targeted_sampling=False, removed_group_df=None, synthpop_ood_df = None):\n",
        "    # Plot distributions of all categorical and continuous features\n",
        "    # Compares train data vs generated (CDTD-CFG & synthpop OOD) vs removed data\n",
        "\n",
        "    n_total = len(cat_features) + len(cont_features)\n",
        "    n_cols = 3\n",
        "    n_rows = (n_total + n_cols - 1) // n_cols\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 3 * n_rows))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, label in enumerate(cat_features):\n",
        "        _plot_cat(df_train, df_generated, label, axes[i], cfg, targeted_sampling, removed_group_df, synthpop_ood_df)\n",
        "    for j, label in enumerate(cont_features):\n",
        "        _plot_cont(df_train, df_generated, label, axes[len(cat_features) + j], cfg, targeted_sampling, removed_group_df, synthpop_ood_df)\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def _plot_cat(df_train, df_generated, label, ax, cfg, targeted_sampling, removed_group_df, synthpop_ood_df):\n",
        "    # Plots categorical feature distribution as bar charts.\n",
        "    # Aligns category labels across train, generated, removed, and synthpop sets.\n",
        "    df_train[label] = df_train[label].astype(str)\n",
        "    df_generated[label] = df_generated[label].astype(str)\n",
        "    if removed_group_df is not None:\n",
        "        removed_group_df = removed_group_df.copy()\n",
        "        removed_group_df[label] = removed_group_df[label].astype(str)\n",
        "    if synthpop_ood_df is not None:\n",
        "        synthpop_ood_df[label] = synthpop_ood_df[label].astype(str)\n",
        "\n",
        "    props_train = df_train[label].value_counts(normalize=True)\n",
        "    props_gen = df_generated[label].value_counts(normalize=True)\n",
        "    props_removed = removed_group_df[label].value_counts(normalize=True) if (cfg and targeted_sampling and removed_group_df is not None) else pd.Series(dtype=float)\n",
        "    props_synth = synthpop_ood_df[label].value_counts(normalize=True) if (cfg and targeted_sampling and synthpop_ood_df is not None) else pd.Series(dtype=float)\n",
        "\n",
        "    all_cats = sorted(set(props_removed.index).union(props_train.index).union(props_gen.index).union(props_synth.index))\n",
        "    x = np.arange(len(all_cats))\n",
        "    y_removed = [props_removed.get(cat, 0.0) for cat in all_cats]\n",
        "    y_train = [props_train.get(cat, 0.0) for cat in all_cats]\n",
        "    y_gen = [props_gen.get(cat, 0.0) for cat in all_cats]\n",
        "    y_synthpop = [props_synth.get(cat, 0.0) for cat in all_cats]\n",
        "\n",
        "    n_bars = 4 if cfg and targeted_sampling else 2\n",
        "    bar_width = 0.8 / n_bars\n",
        "    offset = [-1.5, -0.5, 0.5, 1.5] if n_bars == 4 else [-0.15, 0.15]\n",
        "    if cfg and targeted_sampling:\n",
        "        ax.bar(x + offset[2]*bar_width, y_removed, width=bar_width, label=\"Removed Data\", color=\"tab:orange\")\n",
        "        ax.bar(x + offset[3]*bar_width, y_synthpop, width=bar_width, label=\"Synthpop OOD\", color=\"tab:purple\")\n",
        "    ax.bar(x + offset[0]*bar_width, y_train, width=bar_width, label=\"Train Only Data\", color=\"tab:green\")\n",
        "    ax.bar(x + offset[1]*bar_width, y_gen, width=bar_width, label=\"CDTD-OOD\", color=\"tab:blue\")\n",
        "    ax.set_title(label)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(all_cats, rotation=60, fontsize=7)\n",
        "\n",
        "\n",
        "def _plot_cont(df_train, df_generated, label, ax, cfg, targeted_sampling, removed_group_df, synthpop_ood_df):\n",
        "    # Plot continuous feature distribution using KDE curves.\n",
        "    if cfg and targeted_sampling and removed_group_df is not None and len(removed_group_df) > 0:\n",
        "        sns.kdeplot(removed_group_df[label], ax=ax, label=\"Removed Data\", color=\"tab:orange\")\n",
        "    if cfg and targeted_sampling and synthpop_ood_df is not None and len(synthpop_ood_df) > 0:\n",
        "        sns.kdeplot(synthpop_ood_df[label], ax=ax, label=\"Synthpop OOD\", color=\"tab:purple\")\n",
        "    sns.kdeplot(df_train[label], ax=ax, label=\"Train Only Data\", color=\"tab:green\")\n",
        "    sns.kdeplot(df_generated[label], ax=ax, label=\"CDTD-OOD\", color=\"tab:blue\")\n",
        "    ax.set_title(label)\n",
        "\n",
        "def create_train_test_combinations_ext(\n",
        "    X_cat_train,\n",
        "    X_cont_train,\n",
        "    X_cat_test,\n",
        "    X_cont_test,\n",
        "    df_generated_ID=None,\n",
        "    df_generated_OOD=None,\n",
        "    df_generated_synthpop_ID=None,\n",
        "    df_generated_synthpop_OOD=None,\n",
        "    removed_group_df=None,\n",
        "    cat_features=None,\n",
        "    cont_features=None,\n",
        "    label_1=None,\n",
        "    label_2=None,\n",
        "    target_label_1=None,\n",
        "    target_label_2=None,\n",
        "    classification_variable = None,\n",
        "    classification_target = None,\n",
        "    cat_int_enc=None,\n",
        "    OOD_share=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Build all training dataset combinations:\n",
        "    - train only, +ID, +OOD, +both, + real, and train combined baseline\n",
        "    Decodes encoded train/test sets, adds target column, and filters OOD to target subgroup.\n",
        "    Splits removed data, constructs df_test, and ensures feature type consistency.\n",
        "    Returns dictionary of all train sets, the cleaned test set, and the categorical feature list that is contained in the X set.\n",
        "    \"\"\"\n",
        "\n",
        "    def add_target(df):\n",
        "        df = df.copy()\n",
        "        df[\"target\"] = (df[classification_variable] == classification_target).astype(int)\n",
        "        return df\n",
        "\n",
        "    # Decode train and test data back into data space form and transform into daataframes with original feature names\n",
        "    X_cat_train_decoded = cat_int_enc.inverse_transform(X_cat_train.numpy())\n",
        "    X_cat_test_decoded = cat_int_enc.inverse_transform(X_cat_test.numpy())\n",
        "    X_cont_train_real = cont_enc.inverse_transform(cont_std.inverse_transform(X_cont_train.numpy()))\n",
        "    X_cont_test_real = cont_enc.inverse_transform(cont_std.inverse_transform(X_cont_test.numpy()))\n",
        "\n",
        "    df_train_raw = pd.DataFrame(np.concatenate([X_cat_train_decoded, X_cont_train_real], axis=1),\n",
        "                                columns=cat_features + cont_features)\n",
        "    df_test_raw = pd.DataFrame(np.concatenate([X_cat_test_decoded, X_cont_test_real], axis=1),\n",
        "                               columns=cat_features + cont_features)\n",
        "\n",
        "    df_train_raw[cont_features] = df_train_raw[cont_features].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    df_test_raw[cont_features] = df_test_raw[cont_features].apply(pd.to_numeric, errors=\"coerce\")\n",
        "    df_train_raw = add_target(df_train_raw)\n",
        "    df_test_raw = add_target(df_test_raw)\n",
        "\n",
        "    # Split the removed group over the test set and a part added to the train + real data set\n",
        "    if removed_group_df is not None:\n",
        "        removed_group_df_half_1, removed_group_df_half_2 = train_test_split(\n",
        "            removed_group_df, test_size=(1 - test_split), random_state=42\n",
        "        )\n",
        "        removed_group_df_half_1 = add_target(removed_group_df_half_1)\n",
        "        removed_group_df_half_2 = add_target(removed_group_df_half_2)\n",
        "        df_test = pd.concat([df_test_raw, removed_group_df_half_1], axis=0).reset_index(drop=True)\n",
        "    else:\n",
        "        df_test = df_test_raw.copy()\n",
        "        removed_group_df_half_2 = None\n",
        "\n",
        "    # Add target to synthetic sets\n",
        "    for df in [df_generated_ID, df_generated_OOD, df_generated_synthpop_ID, df_generated_synthpop_OOD]:\n",
        "        if df is not None:\n",
        "            df[\"target\"] = (df[classification_variable] == classification_target).astype(int)\n",
        "\n",
        "    # Filter synthetic OOD rows to contain the deleted subgroup only\n",
        "    df_generated_OOD_filtered = df_generated_OOD[\n",
        "        (df_generated_OOD[label_1] == target_label_1) & (df_generated_OOD[label_2] == target_label_2)\n",
        "    ].copy() if df_generated_OOD is not None else None\n",
        "\n",
        "    df_generated_synthpop_OOD_filtered = df_generated_synthpop_OOD[ # should remove no data, since synthpop exclusively contains target group data\n",
        "        (df_generated_synthpop_OOD[label_1] == target_label_1) & (df_generated_synthpop_OOD[label_2] == target_label_2)\n",
        "    ].copy() if df_generated_synthpop_OOD is not None else None\n",
        "\n",
        "    # Ensure consistent types\n",
        "    for df in [df_generated_ID, df_generated_OOD_filtered,\n",
        "               df_generated_synthpop_ID, df_generated_synthpop_OOD_filtered,\n",
        "               removed_group_df]:\n",
        "        if df is not None:\n",
        "            for col in cont_features:\n",
        "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # Build all combinations\n",
        "    train_combinations = {\n",
        "        \"only_train\": df_train_raw,\n",
        "        \"train_plus_id\": pd.concat([df_train_raw, df_generated_ID], axis=0) if df_generated_ID is not None else None,\n",
        "        \"train_plus_ood\": pd.concat([df_train_raw, df_generated_OOD_filtered.iloc[:int(len(df_train_raw) * OOD_share)]], axis=0) if df_generated_OOD_filtered is not None else None,\n",
        "        \"train_plus_ood_small\": pd.concat([df_train_raw, df_generated_OOD_filtered.iloc[:int(len(df_train_raw) * OOD_share * 0.5)]], axis=0) if df_generated_OOD_filtered is not None else None,\n",
        "        \"train_plus_ood_large\": pd.concat([df_train_raw, df_generated_OOD_filtered.iloc[:int(len(df_train_raw) * OOD_share * 1.5)]], axis=0) if df_generated_OOD_filtered is not None else None,\n",
        "        \"train_plus_both\": pd.concat([df_train_raw, df_generated_ID, df_generated_OOD_filtered.iloc[:int((len(df_train_raw)+len(df_generated_ID)) * OOD_share)]], axis=0)\n",
        "            if df_generated_ID is not None and df_generated_OOD_filtered is not None else None,\n",
        "        \"train_plus_synthpop_id\": pd.concat([df_train_raw, df_generated_synthpop_ID], axis=0)\n",
        "            if df_generated_synthpop_ID is not None else None,\n",
        "        \"train_plus_synthpop_ood\": pd.concat([df_train_raw, df_generated_synthpop_OOD_filtered.iloc[:int(len(df_train_raw) * OOD_share)]], axis=0)\n",
        "            if df_generated_synthpop_OOD_filtered is not None else None,\n",
        "        \"train_plus_synthpop_both\": pd.concat([df_train_raw, df_generated_synthpop_ID, df_generated_synthpop_OOD_filtered.iloc[:int((len(df_train_raw)+len(df_generated_synthpop_ID)) * OOD_share)]], axis=0)\n",
        "            if df_generated_synthpop_ID is not None and df_generated_synthpop_OOD_filtered is not None else None,\n",
        "        \"train_plus_real\": pd.concat([df_train_raw, removed_group_df_half_2], axis=0),\n",
        "        \"train_combined\": df_train_raw,\n",
        "    }\n",
        "\n",
        "    # Finalize test set by splitting into X and y\n",
        "    X_test = df_test.drop(columns=[\"target\", classification_variable])\n",
        "    y_test = df_test[\"target\"]\n",
        "\n",
        "    # Remove classification variable from feature list (since it is now contained in y variable).\n",
        "    cat_features_cleaned = [col for col in cat_features if col != classification_variable]\n",
        "\n",
        "    for col in cat_features_cleaned:\n",
        "        X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "    return train_combinations, X_test, y_test, cat_features_cleaned, df_test\n",
        "\n",
        "def train_classifier(X_train, y_train, cat_features, cont_features, model, model_name):\n",
        "    # Train the classifiers using a preprocessing pipeline.\n",
        "    # Encodes categorical features (OneHot for Logistic Regression, Ordinal otherwise) and scales continuous features and fits the given model.\n",
        "\n",
        "    for col in cat_features:\n",
        "        X_train[col] = X_train[col].astype(str)\n",
        "\n",
        "    if model_name == \"LogisticRegression\":\n",
        "        preprocessor = ColumnTransformer([\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features),\n",
        "            (\"cont\", StandardScaler(), cont_features),\n",
        "        ])\n",
        "\n",
        "    else:\n",
        "        preprocessor = ColumnTransformer([\n",
        "            (\"cat\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1), cat_features),\n",
        "            (\"cont\", StandardScaler(), cont_features),\n",
        "        ])\n",
        "\n",
        "    clf_pipeline = Pipeline([\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"clf\", model)\n",
        "    ])\n",
        "    clf_pipeline.fit(X_train, y_train)\n",
        "    return clf_pipeline\n",
        "\n",
        "def evaluate_classifier(clf_pipeline, X_test, y_test, cat_features):\n",
        "    # Evaluate trained classifier on test data.\n",
        "    # Allows for printing of classification report and confusion matrix for performance insights.\n",
        "    for col in cat_features:\n",
        "        X_test[col] = X_test[col].astype(str)\n",
        "\n",
        "    y_pred = clf_pipeline.predict(X_test)\n",
        "\n",
        "    print(\"=== Classifier Report ===\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cm_df = pd.DataFrame(cm,\n",
        "                         index=[\"Actual 0\", \"Actual 1\"],\n",
        "                         columns=[\"Predicted 0\", \"Predicted 1\"])\n",
        "    print(\"\\n=== Confusion Matrix ===\")\n",
        "    print(cm_df)\n",
        "\n",
        "def generate_samples_synthpop(\n",
        "    synthpop_model,\n",
        "    num_samples,\n",
        "    X_cat_train,\n",
        "    cat_int_enc,\n",
        "    label_1,\n",
        "    label_2,\n",
        "    target_label_1,\n",
        "    target_label_2,\n",
        "    targeted_sampling=True\n",
        "):\n",
        "    # Generate samples using a Synthpop model. Supports targeted OOD sampling (only passing target labels) or ID sampling (copying train distribution).\n",
        "\n",
        "    if targeted_sampling:\n",
        "        # Get label indices and category mappings\n",
        "        label_1_idx = synthpop_model.x_cat_indices[0]\n",
        "        label_2_idx = synthpop_model.x_cat_indices[1]\n",
        "        classes_label_1 = cat_int_enc.categories_[label_1_idx]\n",
        "        classes_label_2 = cat_int_enc.categories_[label_2_idx]\n",
        "\n",
        "        # Find target label indices\n",
        "        target_label_1_idx = np.where(classes_label_1 == target_label_1)[0][0]\n",
        "        target_label_2_idx = np.where(classes_label_2 == target_label_2)[0][0]\n",
        "\n",
        "        # X_fixed_cat is set to target values for all samples for OOD group\n",
        "        X_fixed_cat = np.full((num_samples, 2), fill_value=[target_label_1_idx, target_label_2_idx], dtype=int)\n",
        "        X_fixed_cont = None\n",
        "\n",
        "    else:\n",
        "        # ID sampling we copy the training distribution\n",
        "        X_fixed_cat = X_cat_train[:, synthpop_model.x_cat_indices].numpy()\n",
        "        X_fixed_cont = None\n",
        "\n",
        "    # Sample from Synthpop\n",
        "    x_cat_gen, x_cont_gen = synthpop_model.sample(X_fixed_cat, X_fixed_cont)\n",
        "\n",
        "    # Decode into full DataFrame\n",
        "    df_generated = decode_generated_samples(x_cat_gen, x_cont_gen, cat_int_enc, label_1, label_2)\n",
        "    return df_generated\n",
        "\n",
        "def decode_generated_samples(x_cat_gen, x_cont_gen, cat_int_enc, label_1, label_2):\n",
        "    # Decode raw Synthpop categorical and continuous outputs into a DataFrame.\n",
        "    # Places generated values in the correct feature positions and inverse-transforms categories and adds continuous columns if present.\n",
        "\n",
        "    x_cat_full = np.zeros((x_cat_gen.shape[0], len(cat_int_enc.feature_names_in_)), dtype=int)\n",
        "    label_1_idx = list(cat_int_enc.feature_names_in_).index(label_1)\n",
        "    label_2_idx = list(cat_int_enc.feature_names_in_).index(label_2)\n",
        "    x_cat_full[:, label_1_idx] = x_cat_gen[:, 0]\n",
        "    x_cat_full[:, label_2_idx] = x_cat_gen[:, 1]\n",
        "\n",
        "    decoded = cat_int_enc.inverse_transform(x_cat_full)\n",
        "    df_cats = pd.DataFrame(decoded, columns=cat_int_enc.feature_names_in_)\n",
        "\n",
        "    if x_cont_gen is not None:\n",
        "        # generic names are given to continuous columns to distinguish them from categorical columns\n",
        "        df_conts = pd.DataFrame(x_cont_gen, columns=[f\"cont_{i}\" for i in range(x_cont_gen.shape[1])])\n",
        "        df = pd.concat([df_cats, df_conts], axis=1)\n",
        "    else:\n",
        "        df = df_cats\n",
        "\n",
        "    return df\n",
        "\n",
        "def decode_synthpop_samples(\n",
        "    x_cat_gen,\n",
        "    x_cont_gen,\n",
        "    X_fixed_cat,\n",
        "    label_1_idx,\n",
        "    label_2_idx,\n",
        "    cat_features,\n",
        "    cont_features,\n",
        "    int_features,\n",
        "    cat_int_enc,\n",
        "    cont_enc,\n",
        "    cont_std\n",
        "):\n",
        "    # Rebuilds full sample DataFrame from Synthpop outputs and fixed conditioning variables.\n",
        "    # Restores categorical variables via encoder, inverse-scales continuous values, and rounds ints. Ensures output matches original feature order.\n",
        "\n",
        "    x_cat_full = np.empty((x_cat_gen.shape[0], len(cat_features)), dtype=int)\n",
        "\n",
        "    for i, idx in enumerate([label_1_idx, label_2_idx]):\n",
        "        x_cat_full[:, idx] = X_fixed_cat[:, i]\n",
        "\n",
        "    gen_indices = [i for i in range(len(cat_features)) if i not in [label_1_idx, label_2_idx]]\n",
        "    for i, idx in enumerate(gen_indices):\n",
        "        x_cat_full[:, idx] = x_cat_gen[:, i]\n",
        "\n",
        "    x_cat_gen_inv = cat_int_enc.inverse_transform(x_cat_full)\n",
        "    df_cats = pd.DataFrame(x_cat_gen_inv, columns=cat_features)\n",
        "    x_cont_inv = cont_std.inverse_transform(x_cont_gen)\n",
        "    x_cont_inv = cont_enc.inverse_transform(x_cont_inv)\n",
        "    df_cont = pd.DataFrame(x_cont_inv, columns=cont_features)\n",
        "\n",
        "    df_out = pd.concat([df_cats, df_cont], axis=1)\n",
        "    df_out[int_features] = df_out[int_features].round().astype(int)\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def summarize_target_overlap(df, label_1, label_2, target_label_1, target_label_2, name=\"Dataset\"):\n",
        "    # Print summary of how many rows in a dataset match one or both target labels. Helps understand the distribution of (synthetic) data.\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    cond_1 = df[label_1] == target_label_1\n",
        "    cond_2 = df[label_2] == target_label_2\n",
        "    both_match = (cond_1 & cond_2).sum()\n",
        "    only_1 = (cond_1 & ~cond_2).sum()\n",
        "    only_2 = (~cond_1 & cond_2).sum()\n",
        "    neither = (~cond_1 & ~cond_2).sum()\n",
        "    total = len(df)\n",
        "    print(f\"Total rows: {total}\")\n",
        "    print(f\"Match both [{target_label_1}, {target_label_2}]: {both_match} ({both_match/total:.2%})\")\n",
        "    print(f\"Only {label_1} == {target_label_1}: {only_1} ({only_1/total:.2%})\")\n",
        "    print(f\"Only {label_2} == {target_label_2}: {only_2} ({only_2/total:.2%})\")\n",
        "    print(f\"Match neither: {neither} ({neither/total:.2%})\")\n",
        "\n",
        "def compute_distance_metrics(real_df, synthetic_df, cat_features, cont_features):\n",
        "    # Compare real vs. synthetic datasets using JSD for categorical distributions and Wasserstein distance for continuous variables\n",
        "    # Also checks and reports datatype mismatches between datasets.\n",
        "\n",
        "    type_mismatches = {}\n",
        "    for feature in cat_features + cont_features:\n",
        "        real_dtype = real_df[feature].dtype\n",
        "        synth_dtype = synthetic_df[feature].dtype\n",
        "        if real_dtype != synth_dtype:\n",
        "            type_mismatches[feature] = (real_dtype, synth_dtype)\n",
        "\n",
        "    jsd_results = {}\n",
        "    wsd_results = {}\n",
        "\n",
        "    for feature in cat_features:\n",
        "        real_counts = real_df[feature].astype(str).value_counts(normalize=True)\n",
        "        synth_counts = synthetic_df[feature].astype(str).value_counts(normalize=True)\n",
        "\n",
        "        all_cats = list(set(real_counts.index) | set(synth_counts.index))\n",
        "        real_probs = np.array([real_counts.get(cat, 0) for cat in all_cats])\n",
        "        synth_probs = np.array([synth_counts.get(cat, 0) for cat in all_cats])\n",
        "\n",
        "        real_probs += 1e-12\n",
        "        synth_probs += 1e-12\n",
        "\n",
        "        jsd = jensenshannon(real_probs, synth_probs, base=2)\n",
        "        jsd_results[feature] = jsd\n",
        "\n",
        "    for feature in cont_features:\n",
        "        real_values = pd.to_numeric(real_df[feature], errors=\"coerce\").dropna()\n",
        "        synth_values = pd.to_numeric(synthetic_df[feature], errors=\"coerce\").dropna()\n",
        "        wsd = wasserstein_distance(real_values, synth_values)\n",
        "        wsd_results[feature] = wsd\n",
        "\n",
        "    result_summary = {\n",
        "        \"JSD_mean\": np.mean(list(jsd_results.values())) if jsd_results else None,\n",
        "        \"WSD_mean\": np.mean(list(wsd_results.values())) if wsd_results else None,\n",
        "    }\n",
        "\n",
        "    return result_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fftFQET71cfp",
      "metadata": {
        "id": "fftFQET71cfp"
      },
      "source": [
        "### Dataset preprocessing and hyperparameter initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VTaxs4931Fh3",
      "metadata": {
        "id": "VTaxs4931Fh3"
      },
      "outputs": [],
      "source": [
        "#Setup 1, missing class combination = Circulatory + African American\n",
        "label_1 = \"diagnosis_group\"\n",
        "target_label_1 = \"Circulatory\"\n",
        "\n",
        "# Setup 2, missing class combination = readmitted + African American\n",
        "# label_1 = \"readmitted\"\n",
        "# target_label_1 = \"yes\"\n",
        "\n",
        "target_label_2 = \"AfricanAmerican\"\n",
        "label_2 = \"race\"\n",
        "\n",
        "# I none, label 1 is the classification variable (used in subgroup related classification)\n",
        "classification_target = None\n",
        "classification_variable = None\n",
        "# Can be used in combination with Setup 1 to get the subgroup unrelated classification results\n",
        "# classification_target = \"yes\"\n",
        "# classification_variable = \"readmitted\"\n",
        "\n",
        "# Hyperparameters used in all scenarios throughout the paper under these exact settings.\n",
        "sample_size_scale = 1 # Multiplied by the size of the train data in order to decide how large the synthetic sample will be\n",
        "dropout_ratio = 0.2 # Dropout ratio deciding what part of training is performed unconditionally\n",
        "num_steps_train = 30000 # Number of training steps of the CDTD-CFG MLP\n",
        "num_steps_warmup = 1000 # Number of warmup steps of the CDTD-CFG MLP\n",
        "min_nonmissing_ratio = 0.9 # States that no more than 10% of all categories can be None before being dropped\n",
        "test_split = 0.2 # Ratio of data used for the test set.\n",
        "synthpop_tree_depth = 10 # Max tree depth in synthpop applications\n",
        "min_freq = 0.001 # Minimum frequency of a category before being merged with Other\n",
        "OOD_scale = 1 # Size of the OOD samples we want to create with respects to the training sample size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ikgDI0H2TuM",
      "metadata": {
        "id": "4ikgDI0H2TuM"
      },
      "outputs": [],
      "source": [
        "### Load and clean data\n",
        "\n",
        "df = pd.read_csv(\"data/diabetic_data.csv\")\n",
        "\n",
        "# Keep only the first encounter per patient, drop ID's and shuffle dataset\n",
        "df = df.sort_values(\"encounter_id\").drop_duplicates(\"patient_nbr\", keep=\"first\")\n",
        "df.drop(columns=[\"encounter_id\", \"patient_nbr\"], inplace=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Empty strings, spaces, None, and emtpy are set to NaN\n",
        "df = df.replace(r\"^\\s*$\", np.nan, regex=True)\n",
        "df = df.replace(\"?\", np.nan)\n",
        "df = df.replace(\n",
        "    to_replace=r\"(?i)^\\s*(none|empty|\\?)\\s*$\",\n",
        "    value=np.nan,\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "# Creating count variable of number of diabetes medications taken (excluding insulin), and subsequently drop them\n",
        "diabetes_meds = [\n",
        "    \"metformin\", \"repaglinide\", \"nateglinide\", \"chlorpropamide\", \"glimepiride\",\n",
        "    \"acetohexamide\", \"glipizide\", \"glyburide\", \"tolbutamide\", \"pioglitazone\",\n",
        "    \"rosiglitazone\", \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\",\n",
        "    \"examide\", \"citoglipton\", \"glyburide-metformin\", \"glipizide-metformin\",\n",
        "    \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\"\n",
        "]\n",
        "df[\"num_diabetes_meds\"] = df[diabetes_meds].apply(\n",
        "    lambda row: sum(val in [\"Up\", \"Down\", \"Steady\"] for val in row), axis=1\n",
        ")\n",
        "df.drop(columns=diabetes_meds, inplace=True)\n",
        "\n",
        "# Diagnosis group mapping, to replace three distinct diagnosis by a single group\n",
        "def map_diagnosis_group(code):\n",
        "    try:\n",
        "        code = float(code)\n",
        "    except:\n",
        "        return \"Other\"\n",
        "\n",
        "    if (390 <= code <= 459) or (code == 785):\n",
        "        return \"Circulatory\"\n",
        "    elif (460 <= code <= 519) or (code == 786):\n",
        "        return \"Respiratory\"\n",
        "    elif (520 <= code <= 579) or (code == 787):\n",
        "        return \"Digestive\"\n",
        "    elif code == 250:\n",
        "        return \"Diabetes\"\n",
        "    elif (800 <= code <= 999):\n",
        "        return \"Injury\"\n",
        "    elif (710 <= code <= 739):\n",
        "        return \"Musculoskeletal\"\n",
        "    elif (580 <= code <= 629) or (code == 788):\n",
        "        return \"Genitourinary\"\n",
        "    elif (140 <= code <= 239):\n",
        "        return \"Neoplasms\"\n",
        "    else:\n",
        "        return \"Other\"\n",
        "\n",
        "df[\"diagnosis_group\"] = df[\"diag_1\"].apply(map_diagnosis_group)\n",
        "df.drop(columns=[\"diag_1\", \"diag_2\", \"diag_3\"], inplace=True)\n",
        "\n",
        "\n",
        "# Ethnicity mapping to create a larger Other set\n",
        "def map_etnicity(ethnicity):\n",
        "    if ethnicity in [\"?\", \"Asian\", \"Hispanic\", \"Other\"]:\n",
        "        return \"Other\"\n",
        "    else:\n",
        "        return ethnicity\n",
        "df[\"race\"] = df[\"race\"].apply(map_etnicity)\n",
        "\n",
        "\n",
        "# Transform readmitted to boolean, by settings al readmission variables to yes\n",
        "df[\"readmitted\"] = df[\"readmitted\"].apply(lambda x: \"yes\" if x == \"<30\" or x == \">30\" else \"no\")\n",
        "\n",
        "# Define categorical, continuous, and integer feature lists\n",
        "cat_features = [\n",
        "    \"race\", \"gender\", \"age\", \"admission_type_id\", \"discharge_disposition_id\",\n",
        "    \"payer_code\", \"medical_specialty\", \"max_glu_serum\", \"A1Cresult\",\n",
        "    \"insulin\", \"change\", \"diabetesMed\", \"readmitted\", \"diagnosis_group\"\n",
        "]\n",
        "cont_features = [col for col in df.columns if col not in cat_features]\n",
        "int_features = [\n",
        "    \"admission_source_id\", \"time_in_hospital\", \"num_lab_procedures\",\n",
        "    \"num_procedures\", \"num_medications\", \"number_outpatient\",\n",
        "    \"number_emergency\", \"number_inpatient\", \"number_diagnoses\",\n",
        "    \"num_diabetes_meds\"\n",
        "]\n",
        "\n",
        "# Drop rows with missing diagnosis group and features with too many missing categories\n",
        "df = df[df[\"diagnosis_group\"].notna()]\n",
        "df = df.loc[:, df.notna().mean() >= min_nonmissing_ratio]\n",
        "\n",
        "# Update feature lists to only contain remaining columns\n",
        "cat_features = [col for col in cat_features if col in df.columns]\n",
        "cont_features = [col for col in cont_features if col in df.columns]\n",
        "int_features = [col for col in int_features if col in df.columns]\n",
        "\n",
        "# Now fill remaining categorical NaNs with string 'empty' and drop continuous rows with missing values\n",
        "df[cat_features] = df[cat_features].fillna(\"empty\")\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "for col in cat_features:\n",
        "    freqs = df[col].value_counts(normalize=True)\n",
        "    rare = freqs[freqs < min_freq].index\n",
        "    df[col] = df[col].apply(lambda x: \"other\" if x in rare else x)\n",
        "\n",
        "# Remove the target group from the training set dataset, storing it in a different dataset for later use\n",
        "# We compute the OOD_target (how prevalent was the group in the original dataset), and compute the OOD_Share we need to add on top of the training data to match this target.\n",
        "removed_group_df = df[(df[label_1] == target_label_1) & (df[label_2] == target_label_2)].copy()\n",
        "OOD_target = removed_group_df.shape[0] / df.shape[0]\n",
        "OOD_share_df = OOD_target/(1 - OOD_target)\n",
        "df = df[~((df[label_1] == target_label_1) & (df[label_2] == target_label_2))]\n",
        "\n",
        "# ensure correct types\n",
        "X_cat = df[cat_features].to_numpy().astype(\"str\")\n",
        "X_cont = df[cont_features].to_numpy().astype(\"float\")\n",
        "\n",
        "# Encode categorical and continuous data\n",
        "X_cat_df = df[cat_features].astype(str)\n",
        "cat_int_enc = OrdinalEncoder()\n",
        "X_cat_enc = cat_int_enc.fit_transform(X_cat_df)\n",
        "\n",
        "X_cont = df[cont_features].astype(float).to_numpy()\n",
        "cont_enc = QuantileTransformer(\n",
        "    output_distribution=\"normal\",\n",
        "    n_quantiles=max(min(X_cont.shape[0] // 30, 1000), 10),\n",
        "    subsample=int(1e9),\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "X_cont_enc = cont_enc.fit_transform(X_cont)\n",
        "cont_std = StandardScaler()\n",
        "X_cont_enc_std = cont_std.fit_transform(X_cont_enc)\n",
        "\n",
        "# Convert to PyTorch tensors and split into train and test set\n",
        "X_cat_tensor = torch.tensor(X_cat_enc).long()\n",
        "X_cont_tensor = torch.tensor(X_cont_enc_std).float()\n",
        "\n",
        "X_cat_train, X_cat_test, X_cont_train, X_cont_test = train_test_split(\n",
        "    X_cat_tensor, X_cont_tensor, test_size=test_split, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "X_cat_train = torch.tensor(X_cat_train).long()\n",
        "X_cat_test = torch.tensor(X_cat_test).long()\n",
        "X_cont_train = torch.tensor(X_cont_train).float()\n",
        "X_cont_test = torch.tensor(X_cont_test).float()\n",
        "\n",
        "\n",
        "# Sets hyperparams used in the training and sampling process\n",
        "num_samples = round(X_cat_train.shape[0]) * sample_size_scale\n",
        "label_1_idx = cat_features.index(label_1)\n",
        "label_2_idx = cat_features.index(label_2)\n",
        "OOD_share = OOD_share_df * OOD_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L2fMjvRBEiYk",
      "metadata": {
        "id": "L2fMjvRBEiYk"
      },
      "source": [
        "### CDTD-CFG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t8p8AiK6s2gY",
      "metadata": {
        "id": "t8p8AiK6s2gY"
      },
      "outputs": [],
      "source": [
        "# Train the CDTD-CFG model\n",
        "cdtd_cfg_model = fit_cdtd_model(\n",
        "    X_cat_train=X_cat_train,\n",
        "    X_cont_train=X_cont_train,\n",
        "    cfg=True,\n",
        "    label_1_idx=label_1_idx,\n",
        "    label_2_idx=label_2_idx,\n",
        "    dropout_ratio=dropout_ratio,\n",
        "    num_steps_train=num_steps_train,\n",
        "    num_steps_warmup=num_steps_warmup\n",
        ")\n",
        "\n",
        "# Not used within the final thesis, but shows a CDTD-CFG ID sample call\n",
        "# df_CFG_generated_ID = generate_samples(\n",
        "#     num_samples=round(num_samples),\n",
        "#     cat_int_enc=cat_int_enc,\n",
        "#     cont_enc=cont_enc,\n",
        "#     cont_std=cont_std,\n",
        "#     cat_features=cat_features,\n",
        "#     cont_features=cont_features,\n",
        "#     int_features=int_features,\n",
        "#     cfg=cfg,\n",
        "#     cfg_scale_1=0.4,\n",
        "#     cfg_scale_2=0.4,\n",
        "#     targeted_sampling=False,\n",
        "#     target_label_1=target_label_1,\n",
        "#     target_label_2=target_label_2,\n",
        "#     X_cat_train=X_cat_train,\n",
        "#     label_1=label_1,\n",
        "#     label_2=label_2,\n",
        "#     cdtd_model=cdtd_cfg_model\n",
        "# )\n",
        "\n",
        "# Define guidance weight grid search, saving the combination that yields the best match % for further analysis\n",
        "cfg_scales = np.linspace(0.3, 0.7, 5)\n",
        "match_results = []\n",
        "best_match_percentage = -1\n",
        "df_generated_OOD = None\n",
        "best_cfg_scales = (None, None)\n",
        "\n",
        "# Loop over all combinations\n",
        "for scale_1 in cfg_scales:\n",
        "    for scale_2 in cfg_scales:\n",
        "        print(\"scale_1: \", scale_1, \" scale_2: \", scale_2)\n",
        "        df_generated_OOD_draw, latent_vectors = generate_samples(\n",
        "            num_samples=round(num_samples),\n",
        "            cat_int_enc=cat_int_enc,\n",
        "            cont_enc=cont_enc,\n",
        "            cont_std=cont_std,\n",
        "            cat_features=cat_features,\n",
        "            cont_features=cont_features,\n",
        "            int_features=int_features,\n",
        "            cfg=True,\n",
        "            cfg_scale_1=scale_1,\n",
        "            cfg_scale_2=scale_2,\n",
        "            targeted_sampling=True,\n",
        "            target_label_1=target_label_1,\n",
        "            target_label_2=target_label_2,\n",
        "            X_cat_train=X_cat_train,\n",
        "            label_1=label_1,\n",
        "            label_2=label_2,\n",
        "            cdtd_model=cdtd_cfg_model,\n",
        "            return_latents= False # Can be used for additional analyses, to see how the latent sample affects the data quality\n",
        "        )\n",
        "\n",
        "        # Compute match % and distance metrics\n",
        "        match_mask = (df_generated_OOD_draw[label_1] == target_label_1) & (df_generated_OOD_draw[label_2] == target_label_2)\n",
        "        match_percentage = match_mask.mean() * 100\n",
        "\n",
        "        df_generated_OOD_filtered_draw = df_generated_OOD_draw[\n",
        "            (df_generated_OOD_draw[label_1] == target_label_1) & (df_generated_OOD_draw[label_2] == target_label_2)\n",
        "        ]\n",
        "        metrics_cdtd_draw = compute_distance_metrics(removed_group_df, df_generated_OOD_filtered_draw, cat_features, cont_features)\n",
        "        print(metrics_cdtd_draw)\n",
        "\n",
        "        match_results.append({\n",
        "            \"cfg_scale_1\": scale_1,\n",
        "            \"cfg_scale_2\": scale_2,\n",
        "            \"match_percentage\": match_percentage,\n",
        "            \"JSD_mean\": metrics_cdtd_draw[\"JSD_mean\"],\n",
        "            \"WSD_mean\": metrics_cdtd_draw[\"WSD_mean\"]\n",
        "        })\n",
        "\n",
        "        if match_percentage > best_match_percentage:\n",
        "            best_match_percentage = match_percentage\n",
        "            df_generated_OOD = df_generated_OOD_draw.copy()\n",
        "            best_cfg_scales = (scale_1, scale_2)\n",
        "\n",
        "match_df = pd.DataFrame(match_results)\n",
        "\n",
        "match_table = match_df.pivot(index=\"cfg_scale_1\", columns=\"cfg_scale_2\", values=\"match_percentage\")\n",
        "print(\"\\n=== Match % for each cfg_scale combination ===\\n\")\n",
        "print(match_table.round(2))\n",
        "\n",
        "jsd_table = match_df.pivot(index=\"cfg_scale_1\", columns=\"cfg_scale_2\", values=\"JSD_mean\")\n",
        "print(\"\\n=== JSD_mean for each cfg_scale combination ===\\n\")\n",
        "print(jsd_table.round(4))\n",
        "\n",
        "wsd_table = match_df.pivot(index=\"cfg_scale_1\", columns=\"cfg_scale_2\", values=\"WSD_mean\")\n",
        "print(\"\\n=== WSD_mean for each cfg_scale combination ===\\n\")\n",
        "print(wsd_table.round(4))\n",
        "\n",
        "print(best_cfg_scales,best_match_percentage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DHtF0h0IvCEo",
      "metadata": {
        "id": "DHtF0h0IvCEo"
      },
      "source": [
        "### CDTD-ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "umcCCdNnvBWk",
      "metadata": {
        "id": "umcCCdNnvBWk"
      },
      "outputs": [],
      "source": [
        "# Train the CDTD model\n",
        "cdtd_model = fit_cdtd_model(\n",
        "    X_cat_train=X_cat_train,\n",
        "    X_cont_train=X_cont_train,\n",
        "    cfg=False,\n",
        "    label_1_idx=label_1_idx,\n",
        "    label_2_idx=label_2_idx,\n",
        "    dropout_ratio=dropout_ratio,\n",
        "    num_steps_train=num_steps_train,\n",
        "    num_steps_warmup=num_steps_warmup\n",
        ")\n",
        "\n",
        "# Generate CDTD ID samples\n",
        "df_generated_ID, latent_vectors = generate_samples(\n",
        "    num_samples=round(num_samples),\n",
        "    cat_int_enc=cat_int_enc,\n",
        "    cont_enc=cont_enc,\n",
        "    cont_std=cont_std,\n",
        "    cat_features=cat_features,\n",
        "    cont_features=cont_features,\n",
        "    int_features=int_features,\n",
        "    cfg=False,\n",
        "    cfg_scale_1=0,\n",
        "    cfg_scale_2=0,\n",
        "    targeted_sampling=False,\n",
        "    target_label_1=target_label_1,\n",
        "    target_label_2=target_label_2,\n",
        "    X_cat_train=X_cat_train,\n",
        "    label_1=label_1,\n",
        "    label_2=label_2,\n",
        "    cdtd_model=cdtd_model,\n",
        "    return_latents=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tA9DDdqriDd7",
      "metadata": {
        "id": "tA9DDdqriDd7"
      },
      "source": [
        "### SYNTHPOP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6l8T3NFLGqqI",
      "metadata": {
        "id": "6l8T3NFLGqqI"
      },
      "outputs": [],
      "source": [
        "from synthpop import Synthpop\n",
        "\n",
        "# Indices of the conditioning variables\n",
        "x_cat_indices = [label_1_idx, label_2_idx]\n",
        "\n",
        "# Create and fit the synthpop model (both ID and OOD trees at once)\n",
        "synthpop_model = Synthpop(\n",
        "    cat_train=X_cat_train,\n",
        "    cont_train=X_cont_train,\n",
        "    x_cat_indices=x_cat_indices,\n",
        "    cfg=True,\n",
        "    max_depth=10\n",
        ")\n",
        "\n",
        "_ = synthpop_model.fit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J-LdHw00DgQB",
      "metadata": {
        "id": "J-LdHw00DgQB"
      },
      "outputs": [],
      "source": [
        "# for ID sampling we copy the training conditioning as input for the two target categories\n",
        "X_fixed_cat_ID = X_cat_train[:, x_cat_indices].numpy()\n",
        "X_fixed_cont_ID = None\n",
        "\n",
        "x_cat_gen_ID, x_cont_gen_ID, X_fixed_cat_ID, X_fixed_cont_ID = synthpop_model.sample(\n",
        "    X_fixed_cat=X_fixed_cat_ID,\n",
        "    X_fixed_cont=X_fixed_cont_ID,\n",
        "    targeted_sampling=False\n",
        ")\n",
        "\n",
        "# For OOD generation, we build a conditioning matrix where verey row has the same input of the target combinations\n",
        "label_1_idx = cat_features.index(label_1)\n",
        "label_2_idx = cat_features.index(label_2)\n",
        "classes_label_1 = cat_int_enc.categories_[label_1_idx]\n",
        "classes_label_2 = cat_int_enc.categories_[label_2_idx]\n",
        "\n",
        "target_label_1_idx = np.where(classes_label_1 == target_label_1)[0][0]\n",
        "target_label_2_idx = np.where(classes_label_2 == target_label_2)[0][0]\n",
        "X_fixed_cat_OOD = np.full((round(num_samples), 2), fill_value=[target_label_1_idx, target_label_2_idx], dtype=int)\n",
        "X_fixed_cont_OOD = None\n",
        "\n",
        "x_cat_gen_OOD, x_cont_gen_OOD, X_fixed_cat_OOD, X_fixed_cont_OOD = synthpop_model.sample(\n",
        "    X_fixed_cat=X_fixed_cat_OOD,\n",
        "    X_fixed_cont=X_fixed_cont_OOD,\n",
        "    targeted_sampling=True\n",
        ")\n",
        "\n",
        "# Sampling OOD and ID synthpop data\n",
        "df_generated_synthpop_OOD = decode_synthpop_samples(\n",
        "    x_cat_gen=x_cat_gen_OOD,\n",
        "    x_cont_gen=x_cont_gen_OOD,\n",
        "    X_fixed_cat=X_fixed_cat_OOD,\n",
        "    label_1_idx=label_1_idx,\n",
        "    label_2_idx=label_2_idx,\n",
        "    cat_features=cat_features,\n",
        "    cont_features=cont_features,\n",
        "    int_features=int_features,\n",
        "    cat_int_enc=cat_int_enc,\n",
        "    cont_enc=cont_enc,\n",
        "    cont_std=cont_std\n",
        ")\n",
        "\n",
        "df_generated_synthpop_ID = decode_synthpop_samples(\n",
        "    x_cat_gen=x_cat_gen_ID,\n",
        "    x_cont_gen=x_cont_gen_ID,\n",
        "    X_fixed_cat=X_fixed_cat_ID,\n",
        "    label_1_idx=label_1_idx,\n",
        "    label_2_idx=label_2_idx,\n",
        "    cat_features=cat_features,\n",
        "    cont_features=cont_features,\n",
        "    int_features=int_features,\n",
        "    cat_int_enc=cat_int_enc,\n",
        "    cont_enc=cont_enc,\n",
        "    cont_std=cont_std\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GLFS5gNMcEyd",
      "metadata": {
        "id": "GLFS5gNMcEyd"
      },
      "source": [
        "### Visualize and compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NTod18_DGze1",
      "metadata": {
        "id": "NTod18_DGze1"
      },
      "outputs": [],
      "source": [
        "df_generated_OOD_filtered = df_generated_OOD[\n",
        "    (df_generated_OOD[label_1] == target_label_1) & (df_generated_OOD[label_2] == target_label_2)\n",
        "].copy() if df_generated_OOD is not None else None\n",
        "\n",
        "# Visualization of all feature distributions\n",
        "plot_all_features(df, df_generated_OOD_filtered, cat_features, cont_features, cfg=True, targeted_sampling=True, removed_group_df=removed_group_df, synthpop_ood_df=df_generated_synthpop_OOD)\n",
        "\n",
        "# By uncommeting and re-running in Setup 1, we get the for subgroup-untrelated output\n",
        "# classification_target = \"yes\"\n",
        "# classification_variable = \"readmitted\"\n",
        "\n",
        "# If no distinct classification target, we classify on variable label_1 (Disease = Circulatory, or readmitted = yes)\n",
        "if classification_target == None and classification_variable == None:\n",
        "    classification_variable = label_1\n",
        "    classification_target = target_label_1\n",
        "\n",
        "# Creating all augmentation setups\n",
        "train_combinations, X_test, y_test, cat_features_cleaned, df_test = create_train_test_combinations_ext(\n",
        "    X_cat_train,\n",
        "    X_cont_train,\n",
        "    X_cat_test,\n",
        "    X_cont_test,\n",
        "    df_generated_ID=df_generated_ID,\n",
        "    df_generated_OOD=df_generated_OOD,\n",
        "    df_generated_synthpop_ID=df_generated_synthpop_ID,\n",
        "    df_generated_synthpop_OOD=df_generated_synthpop_OOD,\n",
        "    removed_group_df=removed_group_df,\n",
        "    cat_features=cat_features,\n",
        "    cont_features=cont_features,\n",
        "    label_1=label_1,\n",
        "    label_2=label_2,\n",
        "    target_label_1=target_label_1,\n",
        "    target_label_2=target_label_2,\n",
        "    classification_variable = classification_variable,\n",
        "    classification_target = classification_target,\n",
        "    cat_int_enc=cat_int_enc,\n",
        "    OOD_share=OOD_share\n",
        ")\n",
        "\n",
        "# Initializing all classifiers\n",
        "models_to_evaluate = {\n",
        "    \"XGBoost\": XGBClassifier(max_depth=3, use_label_encoder=False, eval_metric=\"logloss\", random_state=42, verbosity = 0),\n",
        "    \"LogisticRegression\": LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            random_state=42\n",
        "        ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=12,\n",
        "            random_state=42\n",
        "        ),\n",
        "    }\n",
        "\n",
        "results = []\n",
        "histogram_data = {}\n",
        "\n",
        "# For every augmentation strategy, we train and test the classifiers\n",
        "for dataset_name, df_train in train_combinations.items():\n",
        "    if df_train is None:\n",
        "        continue\n",
        "\n",
        "    target_label_2_copy = target_label_2\n",
        "\n",
        "    # For the combined dataset, transform the African American Class into other for both train and test data\n",
        "    if dataset_name == \"train_combined\":\n",
        "        target_label_2_copy = \"Other\"\n",
        "        df_train[label_2] = df_train[label_2].replace(target_label_2, target_label_2_copy)\n",
        "        df_test[label_2] = df_test[label_2].replace(target_label_2, target_label_2_copy).copy()\n",
        "        X_test[label_2] = X_test[label_2].replace(target_label_2, target_label_2_copy).copy()\n",
        "\n",
        "\n",
        "    print(f\"\\n=================== Dataset: {dataset_name} ===================\")\n",
        "\n",
        "    # Summarize the overlap between target subgroups in the training set\n",
        "    summarize_target_overlap(\n",
        "        df_train,\n",
        "        label_1=label_1,\n",
        "        label_2=label_2,\n",
        "        target_label_1=target_label_1,\n",
        "        target_label_2=target_label_2_copy,\n",
        "        name=dataset_name\n",
        "    )\n",
        "\n",
        "    # Split features for training and testing, and ensure all variables have the correct data type\n",
        "    X_train = df_train.drop(columns=[\"target\", classification_variable])\n",
        "    y_train = df_train[\"target\"]\n",
        "    X_test_array = X_test.copy()\n",
        "    y_test_array = y_test.copy()\n",
        "\n",
        "    for col in cat_features_cleaned:\n",
        "        X_train[col] = X_train[col].astype(str)\n",
        "        X_test_array[col] = X_test_array[col].astype(str)\n",
        "    for col in int_features:\n",
        "        X_train[col] = X_train[col].astype(int)\n",
        "        X_test_array[col] = X_test_array[col].astype(int)\n",
        "\n",
        "    # Train and evaluate each classifier\n",
        "    for model_name, model in models_to_evaluate.items():\n",
        "\n",
        "        # Fit classifier and predict labels and probabilities on the test set\n",
        "        clf = train_classifier(X_train.copy(), y_train.copy(), cat_features_cleaned, cont_features, model, model_name)\n",
        "        y_pred = clf.predict(X_test_array)\n",
        "        y_proba = clf.predict_proba(X_test_array)[:, 1]\n",
        "\n",
        "        # Mask for African American subgroup\n",
        "        mask_target = (df_test[label_2] == target_label_2_copy).to_numpy()\n",
        "\n",
        "        # For train only and train plus OOD print the feature importances (reported in results)\n",
        "        if dataset_name in [\"only_train\", \"train_plus_ood\"]:\n",
        "            print(f\"\\nRace importance for {model_name} ({dataset_name}):\")\n",
        "\n",
        "            if model_name == \"XGBoost\":\n",
        "                print(\"Race importance:\", model.feature_importances_[0])\n",
        "\n",
        "            elif model_name == \"RandomForest\":\n",
        "                print(\"Race importance:\", model.feature_importances_[0])\n",
        "\n",
        "            elif model_name == \"LogisticRegression\":\n",
        "                print(\"Race importance (absolute coefficient):\", model.coef_[0][0])\n",
        "                histogram_data[dataset_name] = np.stack([y_proba, y_test_array, mask_target], axis=1)\n",
        "\n",
        "        # Save and print the auc, precision, recall, and F1 for every classifier trained in each augmentation strategy\n",
        "        auc_score = roc_auc_score(y_test_array, y_proba)\n",
        "        precision = precision_score(y_test_array, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_test_array, y_pred, zero_division=0)\n",
        "        accuracy = accuracy_score(y_test_array, y_pred)\n",
        "        f1 = f1_score(y_test_array, y_pred, zero_division=0)\n",
        "        print(f\"\\n----- Classifier: {model_name} === AUC: {auc_score:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | Accuracy: {accuracy:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Model\": model_name,\n",
        "            \"Group\": \"Overall\",\n",
        "            \"AUC\": auc_score,\n",
        "            \"Precision\": precision,\n",
        "            \"Recall\": recall,\n",
        "            \"F1\": f1,\n",
        "            \"Accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "        # Now do the same for subgroups. Showcasing how the model impacts specifically the classifier performance on the target subgroup\n",
        "        if classification_variable != label_1:\n",
        "            mask_target = (df_test[label_2] == target_label_2_copy) & (df_test[label_1] == target_label_1)\n",
        "        else:\n",
        "            mask_target = df_test[label_2] == target_label_2_copy\n",
        "        mask_non_target = ~mask_target\n",
        "\n",
        "        for is_target, mask in [(True, mask_target), (False, mask_non_target)]:\n",
        "            mask_name = f\"{label_2} == {target_label_2_copy}\" if is_target else f\"{label_2} != {target_label_2_copy}\"\n",
        "            group_name = \"Target\" if is_target else \"Non-Target\"\n",
        "            y_true_subset = y_test_array[mask]\n",
        "            y_pred_subset = y_pred[mask]\n",
        "            y_proba_subset = y_proba[mask]\n",
        "\n",
        "            auc_subset = roc_auc_score(y_true_subset, y_proba_subset)\n",
        "            precision_subset = precision_score(y_true_subset, y_pred_subset, zero_division=0)\n",
        "            recall_subset = recall_score(y_true_subset, y_pred_subset, zero_division=0)\n",
        "            accuracy_subset = accuracy_score(y_true_subset, y_pred_subset)\n",
        "            f1_subset = f1_score(y_true_subset, y_pred_subset, zero_division=0)\n",
        "            print(f\"\\n--- Subset: {mask_name} scores === AUC: {auc_subset:.4f} | Precision: {precision_subset:.4f} | Recall: {recall_subset:.4f} | Accuracy: {accuracy_subset:.4f} | F1: {f1_subset:.4f}\")\n",
        "\n",
        "            results.append({\n",
        "                \"Dataset\": dataset_name,\n",
        "                \"Model\": model_name,\n",
        "                \"Group\": group_name,\n",
        "                \"AUC\": auc_subset,\n",
        "                \"Precision\": precision_subset,\n",
        "                \"Recall\": recall_subset,\n",
        "                \"F1\": f1_subset,\n",
        "                \"Accuracy\": accuracy_subset\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dt-LFoK2y103",
      "metadata": {
        "id": "dt-LFoK2y103"
      },
      "outputs": [],
      "source": [
        "# Summarized classifier outputs used in results seciton.\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "group_names = [\"Overall\", \"Target\"]\n",
        "metrics = [\"AUC\", \"Recall\", \"Precision\", \"F1\", \"Accuracy\"]\n",
        "\n",
        "dataset_order = [\n",
        "    \"only_train\",\n",
        "    \"train_combined\",\n",
        "    \"train_plus_synthpop_id\",\n",
        "    \"train_plus_id\",\n",
        "    \"train_plus_synthpop_ood\",\n",
        "    \"train_plus_ood\",\n",
        "    \"train_plus_synthpop_both\",\n",
        "    \"train_plus_both\",\n",
        "    \"train_plus_real\",\n",
        "    \"train_plus_ood_small\",\n",
        "    \"train_plus_ood_large\",\n",
        "]\n",
        "\n",
        "# The results are displayed for both the overall dataset and the target class (African American) only\n",
        "for group in group_names:\n",
        "    print(f\"\\n\\n========== Group: {group} ==========\\n\")\n",
        "\n",
        "    df_group = results_df[results_df[\"Group\"] == group]\n",
        "\n",
        "    formatted = []\n",
        "    for model in df_group[\"Model\"].unique():\n",
        "        df_model = df_group[df_group[\"Model\"] == model]\n",
        "        df_model = df_model.set_index(\"Dataset\").sort_index()\n",
        "        for metric in metrics:\n",
        "            row_label = (model, metric)\n",
        "            row_values = df_model[metric].to_dict()\n",
        "            formatted.append(pd.Series(row_values, name=row_label))\n",
        "\n",
        "    result_table = pd.DataFrame(formatted).reindex(columns=dataset_order)\n",
        "    result_table.index = pd.MultiIndex.from_tuples(result_table.index, names=[\"Model\", \"Metric\"])\n",
        "\n",
        "    display(result_table)\n",
        "\n",
        "    # A summarized dataset is created, that computes the mean and std over the three classifiers for each strategy\n",
        "    summary_df = df_group.groupby(\"Dataset\")[metrics].agg([\"mean\", \"std\"])\n",
        "    summary_formatted = pd.DataFrame(index=metrics, columns=dataset_order)\n",
        "\n",
        "    for dataset in dataset_order:\n",
        "        for metric in metrics:\n",
        "            mean_val = summary_df.loc[dataset, (metric, \"mean\")]\n",
        "            std_val = summary_df.loc[dataset, (metric, \"std\")]\n",
        "            summary_formatted.loc[metric, dataset] = f\"{mean_val:.3f}  ({std_val:.3f})\"\n",
        "\n",
        "    summary_formatted.index.name = \"Metric\"\n",
        "    print(f\"\\n--- Summary (mean ± std across models) for group: {group} ---\")\n",
        "    display(summary_formatted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-s5YAiE0HHZ9",
      "metadata": {
        "id": "-s5YAiE0HHZ9"
      },
      "outputs": [],
      "source": [
        "# Compute distance metrics for generated OOD samples of synthpop and CDTD-CFG to the removed subgroup\n",
        "metrics_cdtd = compute_distance_metrics(removed_group_df, df_generated_OOD_filtered, cat_features, cont_features)\n",
        "metrics_synthpop = compute_distance_metrics(removed_group_df, df_generated_synthpop_OOD, cat_features, cont_features)\n",
        "\n",
        "print(\"CDTD-CFG OOD: \", metrics_cdtd)\n",
        "print(\"Synthpop OOD: \", metrics_synthpop)\n",
        "\n",
        "# Decode training data back to original feature space in order to compute distance metrics for the training data, and two feature specific subsets too\n",
        "X_cat_train_decoded = cat_int_enc.inverse_transform(X_cat_train.numpy())\n",
        "X_cont_train_real = cont_enc.inverse_transform(cont_std.inverse_transform(X_cont_train.numpy()))\n",
        "df_train_raw = pd.DataFrame(np.concatenate([X_cat_train_decoded, X_cont_train_real], axis=1),\n",
        "                            columns=cat_features + cont_features)\n",
        "df_train_raw[cont_features] = df_train_raw[cont_features].apply(pd.to_numeric, errors=\"coerce\")\n",
        "df_train_raw[int_features] = df_train_raw[int_features].apply(pd.to_numeric, errors=\"coerce\").astype(int)\n",
        "\n",
        "df_train_filtered_label_1 = df_train_raw[\n",
        "    (df_train_raw[label_1] == target_label_1)\n",
        "].copy()\n",
        "df_train_filtered_label_2 = df_train_raw[\n",
        "    (df_train_raw[label_2] == target_label_2)\n",
        "].copy()\n",
        "\n",
        "metrics_train = compute_distance_metrics(removed_group_df, df_train_raw, cat_features, cont_features)\n",
        "metrics_train_label_1 = compute_distance_metrics(removed_group_df, df_train_filtered_label_1, cat_features, cont_features)\n",
        "metrics_train_label_2 = compute_distance_metrics(removed_group_df, df_train_filtered_label_2, cat_features, cont_features)\n",
        "print(\"Train: \", metrics_train)\n",
        "print(\"Train_label_1: \", metrics_train_label_1)\n",
        "print(\"Train_label_2: \", metrics_train_label_2)\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Plots histograms for showcasing the predicted probability of being classified as target = 1 for train only and train + OOD\n",
        "bin_width_main = 0.0333333333333\n",
        "bins_main = np.arange(0, 1 + bin_width_main, bin_width_main)\n",
        "\n",
        "# An additional zoomed in histogram was created to show the distribution in the trainonly scenarios\n",
        "bin_width_zoom = 0.000125\n",
        "bins_zoom = np.arange(0, 0.005 + bin_width_zoom, bin_width_zoom)\n",
        "\n",
        "for dataset_name, data in histogram_data.items():\n",
        "    y_proba, y_true, mask = data[:, 0], data[:, 1], data[:, 2]\n",
        "\n",
        "    # Create boolean masks to seperate between instances where the true target value = 1 and 0\n",
        "    is_masked = (mask == 1)\n",
        "    is_not_masked = (mask == 0)\n",
        "    def get_group_data(mask_filter):\n",
        "        return {\n",
        "            'y0': y_proba[np.logical_and(mask_filter, y_true == 0)],\n",
        "            'y1': y_proba[np.logical_and(mask_filter, y_true == 1)]\n",
        "        }\n",
        "    masked = get_group_data(is_masked)\n",
        "    not_masked = get_group_data(is_not_masked)\n",
        "\n",
        "    def plot_grouped_histogram(group_1, group_2, bins, title_suffix, dataset_name):\n",
        "        counts_1_y0, _ = np.histogram(group_1['y0'], bins)\n",
        "        counts_1_y1, _ = np.histogram(group_1['y1'], bins)\n",
        "        counts_2_y0, _ = np.histogram(group_2['y0'], bins)\n",
        "        counts_2_y1, _ = np.histogram(group_2['y1'], bins)\n",
        "\n",
        "        bin_centers = bins[:-1] + (bins[1] - bins[0]) / 2\n",
        "        bar_width = (bins[1] - bins[0]) / 3\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.bar(bin_centers - bar_width, counts_1_y0, width=bar_width, label='African American: not readmitted', alpha=0.7)\n",
        "        plt.bar(bin_centers - bar_width, counts_1_y1, width=bar_width, bottom=counts_1_y0, label='African American: readmitted', alpha=0.7)\n",
        "\n",
        "        plt.bar(bin_centers + bar_width, counts_2_y0, width=bar_width, label='Non African American: not readmitted', alpha=0.7)\n",
        "        plt.bar(bin_centers + bar_width, counts_2_y1, width=bar_width, bottom=counts_2_y0, label='Non African American: readmitted', alpha=0.7)\n",
        "\n",
        "        plt.title(f'Predicted Probability Histogram - {dataset_name} ({title_suffix})')\n",
        "        plt.xlabel('Predicted Probability (y_proba)')\n",
        "        plt.ylabel('Count')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Plot complete and zoomed-in histograms\n",
        "    plot_grouped_histogram(masked, not_masked, bins_main, 'Full Range', dataset_name)\n",
        "    if dataset_name == \"only_train\":\n",
        "        plot_grouped_histogram(masked, not_masked, bins_zoom, 'Zoom: [0.0–0.005]', dataset_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (CDTD)",
      "language": "python",
      "name": "cdtd_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 0.202493,
      "end_time": "2025-05-09T13:37:23.832652",
      "environment_variables": {},
      "exception": null,
      "input_path": "main.ipynb",
      "output_path": "output.ipynb",
      "parameters": {},
      "start_time": "2025-05-09T13:37:23.630159",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
